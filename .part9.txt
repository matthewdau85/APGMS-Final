        orgId: context.orgId,
        accountId: input.accountId,
        amount: input.amount,
        source: input.source,
        actorId: context.actorId,
      },
    );
  }

  async simulateDebitAttempt(
    context: BankingProviderContext,
    input: CreditDesignatedAccountInput,
  ): Promise<never> {
    try {
      await applyDesignatedAccountTransfer(
        {
          prisma: context.prisma,
          auditLogger: context.auditLogger,
        },
        {
          orgId: context.orgId,
          accountId: input.accountId,
          amount: -Math.abs(input.amount),
          source: input.source,
          actorId: context.actorId,
        },
      );
    } catch (error) {
      if (
        error instanceof AppError &&
        error.code === "designated_withdrawal_attempt"
      ) {
        throw error;
      }

      throw new AppError(
        409,
        "banking_debit_blocked",
        "Debit attempt blocked by provider policy",
      );
    }

    throw new AppError(
      500,
      "debit_policy_error",
      "Debit attempt unexpectedly passed policy checks",
    );
  }
}



============================================================
FILE: C:\src\apgms-final\providers\banking\index.ts
============================================================
import { AnzBankingProvider } from "./anz.js";
import { MockBankingProvider } from "./mock.js";
import { NabBankingProvider } from "./nab.js";
import type { BankingProvider, BankingProviderId } from "./types.js";

export * from "./types.js";

export function createBankingProvider(
  id: BankingProviderId | string,
): BankingProvider {
  const normalized = id.toLowerCase();
  switch (normalized) {
    case "nab":
      return new NabBankingProvider();
    case "anz":
      return new AnzBankingProvider();
    case "mock":
    default:
      return new MockBankingProvider();
  }
}




============================================================
FILE: C:\src\apgms-final\providers\banking\mock.ts
============================================================
import { BaseBankingProvider } from "./base.js";
import type { BankingProviderCapabilities } from "./types.js";

const CAPABILITIES: BankingProviderCapabilities = {
  maxReadTransactions: 200,
  maxWriteCents: 1_000_000,
};

export class MockBankingProvider extends BaseBankingProvider {
  constructor() {
    super("mock", CAPABILITIES);
  }
}




============================================================
FILE: C:\src\apgms-final\providers\banking\nab.ts
============================================================
import { BaseBankingProvider } from "./base.js";
import type { BankingProviderCapabilities } from "./types.js";

const CAPABILITIES: BankingProviderCapabilities = {
  maxReadTransactions: 1000,
  maxWriteCents: 5_000_000,
};

export class NabBankingProvider extends BaseBankingProvider {
  constructor() {
    super("nab", CAPABILITIES);
  }
}




============================================================
FILE: C:\src\apgms-final\providers\banking\types.ts
============================================================
import type { PrismaClient } from "@prisma/client";

import type {
  ApplyDesignatedTransferResult,
  ApplyDesignatedTransferInput,
} from "@apgms/domain-policy";

export type BankingProviderId = "nab" | "anz" | "mock";

export type BankingProviderCapabilities = {
  maxReadTransactions: number;
  maxWriteCents: number;
};

export type BankingProviderContext = {
  prisma: PrismaClient;
  orgId: string;
  actorId: string;
  auditLogger?: (entry: {
    orgId: string;
    actorId: string;
    action: string;
    metadata: Record<string, unknown>;
  }) => Promise<void>;
};

export type CreditDesignatedAccountInput = Omit<
  ApplyDesignatedTransferInput,
  "orgId" | "actorId"
>;

export interface BankingProvider {
  readonly id: BankingProviderId;
  readonly capabilities: BankingProviderCapabilities;

  creditDesignatedAccount(
    context: BankingProviderContext,
    input: CreditDesignatedAccountInput,
  ): Promise<ApplyDesignatedTransferResult>;

  simulateDebitAttempt(
    context: BankingProviderContext,
    input: CreditDesignatedAccountInput,
  ): Promise<never>;
}




============================================================
FILE: C:\src\apgms-final\README.md
============================================================
APGMS â€“ Automated PAYGW & GST Management System

Status: Prototype â€¢ AU-only â€¢ Designed for ATO DSP Operational Security Framework (OSF) alignment
Scope: PAYGW, GST, BAS automation, one-way designated accounts, regulator view, audit guarantees.

APGMS is an Australian-only platform providing hardened, DSP-grade management of PAYGW and GST liabilities, including:

Automated PAYGW/GST calculation using versioned configuration tables.

Designated one-way accounts ensuring PAYGW/GST cannot be misused as operating cash.

ATO-grade audit trails (idempotency, immutable ledgers, shortfall tracking, reconciliation).

Regulator views and APIs consistent with ATO DSP OSF requirements.

End-to-end encryption of TFN/ABN/BSB/account numbers using AES-256-GCM envelopes.

Monorepo Structure
APGMS-Final/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api-gateway/      # Fastify API gateway, auth, MFA, security headers, regulator routes
â”‚   â””â”€â”€ connectors/       # Bank/ATO provider scaffolding (mock + real)
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ domain-policy/    # AU tax engines, designated account policy, rules
â”‚   â””â”€â”€ ledger/           # Double-entry ledger engine, journal & tests
â”œâ”€â”€ shared/               # Shared Prisma schema, client, cross-cutting utils (crypto, redaction)
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ phase1-demo/      # UI demo for Phase 1 flows
â”œâ”€â”€ webapp/               # Main UI (React/Vite)
â”œâ”€â”€ worker/               # Background jobs (parameter updates, projections)
â””â”€â”€ artifacts/            # KMS keys, compliance evidence bundles (gitignored except .gitkeep)

AU-Only Tax Scope

APGMS limits itself to the Australian tax system only for tightness of compliance and correctness:

BAS, PAYGW and GST are implemented through versioned config tables.

Engines do not hardcode thresholds or rates.

Future extensions (PAYGI, FBT, company tax) remain AU-only.

Aligns with ATO's DSP Operational Security Framework by design.

Prerequisites
Required

Node.js 20.11.x
(Use .nvmrc or .tool-versions; supports nvm use or asdf install)

PNPM 9 via Corepack

corepack enable
corepack prepare pnpm@9 --activate


Docker + Docker Compose

PostgreSQL 15/16

Playwright browsers

pnpm exec playwright install --with-deps

Optional

k6 for load testing

Environment Configuration

Create .env files where needed (root, services/api-gateway, etc).

API Gateway startup requirements (hard-fail if missing):

CORS_ALLOWED_ORIGINS must be set (comma-separated).

KMS keyset must be present or API refuses to start.

Example .env:
CORS_ALLOWED_ORIGINS=http://localhost:5173,http://127.0.0.1:5173
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/apgms?schema=public
SHADOW_DATABASE_URL=postgresql://postgres:postgres@localhost:5432/apgms_shadow?schema=public

AUTH_AUDIENCE=urn:apgms:local
AUTH_ISSUER=urn:apgms:issuer
AUTH_DEV_SECRET=local-dev-shared-secret-change-me
AUTH_JWKS={"keys":[{"kid":"local","alg":"RS256","kty":"RSA","n":"replace-with-base64url-modulus","e":"AQAB"}]}

ENCRYPTION_MASTER_KEY=AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=

API_RATE_LIMIT_MAX=120
API_RATE_LIMIT_WINDOW=1 minute
AUTH_FAILURE_THRESHOLD=5

WEBAUTHN_RP_ID=localhost
WEBAUTHN_RP_NAME=APGMS Admin
WEBAUTHN_ORIGIN=http://localhost:5173

REGULATOR_ACCESS_CODE=regulator-dev-code
REGULATOR_JWT_AUDIENCE=urn:apgms:regulator
REGULATOR_SESSION_TTL_MINUTES=60

BANKING_PROVIDER=mock
BANKING_MAX_READ_TRANSACTIONS=1000
BANKING_MAX_WRITE_CENTS=5000000

KMS Keying

Store dev keys in artifacts/kms/*.json (ignored in git).

Rotate keys:

pnpm security:rotate-keys --write-env .env

Quickstart (Local)
pnpm i --frozen-lockfile
pnpm -r build
docker compose up -d
pnpm -w exec prisma migrate deploy
pnpm --filter @apgms/api-gateway dev


API runs on http://localhost:3000
.

Smoke test
curl -sf http://localhost:3000/health
curl -sf http://localhost:3000/ready
curl -sf http://localhost:3000/metrics

Idempotency (ATO-grade)

All mutating endpoints require:

Idempotency-Key: <uuid or ULID>


Behaviour:

First request stores response in immutable log.

Replays return the exact same payload plus:

Idempotent-Replay: true


Reuse with different payload, org, or actor â†’
409 idempotency_conflict

Persist keys across retries.

Quality & Security Gates

These replicate CI locally. Run before pushing.

Type Safety
pnpm -r typecheck

Unit tests + coverage gate (>= 85%)
pnpm -r test -- --coverage
node ./scripts/check-coverage.mjs

Dependency SCA
pnpm audit --audit-level=high

Secret scanning
./gitleaks detect --no-color --redact --exit-code 1

SBOM generation
pnpm sbom        # output sbom.json

Schema drift
pnpm -w exec prisma migrate status

Merge-conflict guard
git grep -n '<<<<<<<\|=======\|>>>>>>>' -- ':!*.lock'

Accessibility & Performance
WCAG Smoke
pnpm -w exec playwright test webapp/tests/a11y.spec.ts

Axe per-route
pnpm --filter @apgms/webapp test:axe

(Optional) Lighthouse
pnpm -w exec lhci autorun --config=./lighthouserc.json

Operational Smoke Tests

Once API is running:

pnpm k6:smoke -- --env BASE_URL=http://localhost:3000

Release Compliance Workflow

A typical evidence bundle:

STAMP=$(date +%Y-%m-%dT%H%M%S)
OUT=artifacts/compliance/$STAMP && mkdir -p "$OUT"

pnpm -r test -- --coverage && cp -r coverage "$OUT/coverage"
node ./scripts/check-coverage.mjs 2>&1 | tee "$OUT/coverage_gate.txt"

pnpm audit --audit-level=high 2>&1 | tee "$OUT/sca.txt"
./gitleaks detect --no-color --redact --exit-code 1 2>&1 | tee "$OUT/gitleaks.txt" || true

pnpm sbom && mv sbom.json "$OUT/sbom.json"

pnpm -w exec prisma migrate status 2>&1 | tee "$OUT/prisma_status.txt"

curl -sSf http://localhost:3000/ready -o "$OUT/ready.json"
curl -sSf http://localhost:3000/metrics -o "$OUT/metrics.prom" || true

Pushing Changes
git checkout -b hardening/compliance-5-2
git add -A
git commit -m "docs(readme): align local workflow with 5.2 compliance gates"
git push -u origin hardening/compliance-5-2


============================================================
FILE: C:\src\apgms-final\runbooks\ndb.md
============================================================
# Notifiable Data Breach (NDB) Runbook

This runbook guides the incident commander through the steps required when a potential Notifiable Data Breach (NDB) is identified.

## 1. Confirm the incident
- Validate the report with the on-call security engineer.
- Gather scope: affected systems, records, and time window.
- Determine whether the incident triggers the NDB scheme (serious harm test).

## 2. Contain and preserve evidence
- Isolate impacted infrastructure while maintaining availability for critical services.
- Capture forensic artifacts (logs, snapshots, database exports) and store them in the incident bucket.
- Rotate credentials that may have been exposed.

## 3. Escalate internally
- Page the executive sponsor (COO) and security lead.
- Spin up the incident channel `#inc-ndb-<date>` with the communication lead and legal counsel.
- Assign owners for customer comms, regulator comms, forensics, and remediation.

## 4. Prepare notifications
Complete these templates before sending and store drafts in the incident drive.

### 4.1 Customer notification (email)
```
Subject: Important security update regarding your Birchal account

Hi <Customer Name>,

We recently detected unauthorised access to a Birchal system that may have exposed information linked to your organisation <Org Name>. The data affected includes <Data Types>. At this time we have no evidence of misuse.

What we are doing
- Contained the incident at <Containment Time>.
- Enabled enhanced monitoring and rotated all relevant credentials.

What you should do
- Reset passwords for Birchal-connected accounts.
- Review recent activity for anything unfamiliar and report anything suspicious to us immediately.

We have reported the incident to the OAIC in accordance with the Notifiable Data Breaches scheme. If you have questions please reply to this email or call our hotline on <Hotline Number>.

Regards,
Birchal Security Team
```

### 4.2 Regulator notification (OAIC portal)
```
Incident summary: <One-line description>
Date discovered: <UTC timestamp>
Likely harm: <Risk summary>
Containment actions: <Short list>
Number of individuals affected: <Count/estimate>
Points of contact: <Legal counsel + incident commander>
Supporting documents: Upload incident timeline and preliminary impact assessment.
```

### 4.3 Internal announcement (Slack)
```
Heads-up: we have declared an NDB incident (INC-<number>). All external communications must be coordinated through #inc-ndb-<date>. Do not share customer names or incident details outside that channel. Next update at <time>.
```

## 5. Execute notifications
- Send regulator notification within 72 hours of confirmation.
- Email impacted customers after legal review and before public disclosure.
- Update the status page template with an incident summary and remediation steps.

## 6. Post-incident activities
- Run the full retrospective within 7 days.
- Capture lessons learned and preventive actions in Jira.
- Update affected runbooks, playbooks, and controls.



============================================================
FILE: C:\src\apgms-final\runbooks\ops.md
============================================================
# Ops Runbook

## Key Endpoints
- `/health` - liveness. Returns 200 when the process is up.
- `/ready` - readiness. Returns 200 only when Postgres plus any configured Redis/NATS dependencies respond; returns 503 (with component detail) while draining or if a dependency is down.
- `/metrics` - Prometheus scrape point. Exposes `apgms_http_requests_total`, `apgms_http_request_duration_seconds`, `apgms_db_query_duration_seconds`, `apgms_db_queries_total`, and `apgms_job_duration_seconds` (`services/api-gateway/src/observability/metrics.ts`).
- `/bank-lines` - authenticated PAYGW/GST ingestion. Requires JWT + allowed role; GET lists caller-org lines, POST accepts ciphertext payloads.
- `/admin/data` - authenticated stub for admin datasets. In-memory only; used for smoke validation of auth plumbing, not for real exports.
- `/tax/health` - authenticated proxy that pings the configured tax engine URL with a short timeout for observability.

## Dependency checks before bring-up
- Confirm the default containers are running (or point the env vars at reachable services):
  - `docker compose up -d db redis` exposes Postgres on `localhost:5432` and Redis on `localhost:6379`.
  - If you rely on external infrastructure, export `DATABASE_URL`, `SHADOW_DATABASE_URL`, and `REDIS_URL` with those hosts before starting the gateway.
- Sanity check connectivity **before** launching `pnpm --filter @apgms/api-gateway dev`:
  - `pg_isready -h <host> -p 5432` (or `psql "$DATABASE_URL" -c "select 1"`) must succeed or Prisma will raise `PrismaClientInitializationError: Can't reach database server` on `/compliance/report`, `/admin/export/:org`, etc.
  - `redis-cli -u $REDIS_URL ping` (or `redis-cli -h localhost -p 6379 ping`) must return `PONG`; otherwise Fastify will log `redis_client_error` for every request that touches rate limits/session stores.
- Restart the gateway after changing any of these endpoints so Fastify picks up the new sockets.

## Start/Stop Procedures
- **Start**: `pnpm --filter @apgms/api-gateway dev`.
- Ensure Postgres is listening on `localhost:5432` (or that `DATABASE_URL`/`SHADOW_DATABASE_URL` point to the actual host) before running the start command. Bringing up the default `db`/`redis` containers via `docker compose up -d db redis` and confirming `docker ps` shows `apgms-db` healthy is the recommended flow.
- **Graceful shutdown**: send SIGTERM/SIGINT; the handler drains and calls `fastify.close()` (`services/api-gateway/src/index.ts`). Verify shutdown by checking the logs for an `api-gateway shut down cleanly` line.

## Auth & Routing Notes
- `buildServer` now registers `/bank-lines`, `/admin/data`, and `/tax/*` inside a Fastify scope that attaches `authGuard`, so every domain route receives `request.user` populated before its own role/policy checks run (`services/api-gateway/src/app.ts`).
- The admin/tax routers still invoke `authenticateRequest` for audit metrics; ensure `AUTH_JWKS`, `AUTH_AUDIENCE`, and `AUTH_ISSUER` are present in every deployment so those hooks succeed.

## Common Alerts
- Surge in `apgms_http_request_duration_seconds` or `apgms_http_requests_total{status="5xx"}` indicates upstream latency or crash loops; validate `/ready` and inspect Fastify logs with trace IDs.
- Non-zero `apgms_db_query_duration_seconds` p95 above baseline usually means Postgres contention; check slow query logs and Prisma instrumentation.
- If `/tax/health` returns 502, the upstream tax engine URL configured in `TAX_ENGINE_URL` is unreachable or returning non-2xx responses.

## Data Handling Caveats
- There is no automated subject deletion/export flow. For right-to-erasure or evidence requests, involve the compliance team and run manual SQL/audit procedures; document every step until the API gains durable routes.
- `/admin/data` is intentionally ephemeral - do not rely on it for long-lived admin artifacts.

## Rolling Deploy / Graceful Shutdown
- `SIGTERM` sets `draining=true`, causing `/ready` to flip to 503 so the load balancer can drain the instance.
- Fastify waits for inflight requests before closing; keep the pod alive until `/ready` reports `{ ok: false, draining: true }` for at least one scrape interval.

## Regulator Access
- Only the regulator auth/session bootstrap is wired today. Evidence catalogue, monitoring snapshots, and bank summary routes remain TODO; direct auditors to the compliance backlog for progress before promising those controls.

- **Compliance Monitoring & Designated Accounts**
- Payroll/POS ingestion now channels data into `PayrollContribution` and `PosTransaction` via `/ingest/payroll` and `/ingest/pos` (use Idempotency-Key headers). The nightly worker applies these rows to the `PAYGW_BUFFER`/`GST_BUFFER` ledgers and validates balances using `ensureDesignatedAccountCoverage` (`shared/src/ledger/designated-account.ts`), raising `DESIGNATED_FUNDS_SHORTFALL` alerts when requirements exceed available funds.
- `/compliance/precheck`, `/compliance/status`, `/compliance/pending`, and `/compliance/reminders` expose readiness, buffer snapshots, pending contributions, and guidance for remediation. Use these endpoints to re-ingest missing batches, rerun the reconciliation job, and capture the proof in the audit log before notifying the business or the ATO.
- When the shortfall alert resolves, call `/compliance/alerts/:id/resolve`, log the remediation evidence, and file it under `artifacts/compliance/` (include OSF exports, correspondence, adapter plans). Automated reminders should flag upcoming BAS deadlines and penalties while the system records remission/payment-plan conversations.

- **Regulatory Status**
- Track the ATO DSP OSF questionnaire, product registration, AUSTRAC/ASIC discussions, and the ADI/banking partner plan inside `docs/runbooks/admin-controls.md` (or a dedicated `status/` note). Keep copies of submissions/contracts in `artifacts/compliance/` so the legal team can show evidence when auditors arrive.
- **Partnering & Pilots**
- After you select a banking partner/adaptor, configure `DESIGNATED_BANKING_URL`/`DESIGNATED_BANKING_TOKEN` (or call `configureBankingAdapter`) so the ledger queries the sandbox endpoint. Record the partnerâ€™s API spec, certificate chain, and test accounts in `artifacts/compliance/`.
- Log the DSP Product ID, OSF questionnaire ID, and AUSTRAC/ASIC/AFSL path within the compliance dashboard and runbook. Run pilots by feeding payroll/POS batches through `/ingest/*`, calling `/compliance/precheck`, and capturing alert resolution evidence via `/compliance/status` + `/compliance/alerts/:id/resolve`. Document each pilot organisation, payload trace, precheck response, and resolution timeline for audit reviewers.
- **Innovation Signals**
- `/compliance/status` adds `forecast` and `tierStatus` so you can treat the forecasted obligations as â€œvirtual balancesâ€ and trigger warnings when the tier drops to `escalate`. Surface these signals on your dashboard/alerting playbooks to flag unusual shortfalls before BAS lodgment.
- Log the heuristic you use for `forecastObligations`/`computeTierStatus` so regulators can understand the predictive engine even if itâ€™s just historical averages; store the tuning notes in `artifacts/compliance/`.
- **Stakeholder Connection**
- Follow `docs/runbooks/stakeholder-connect.md` for the first-run checklist: populate `DESIGNATED_BANKING_*` plus `DSP_PRODUCT_ID`, run through the pilot steps, and ship the generated `artifacts/compliance/partner-info.json` + pilot report to your external partner/regulator. Update this located doc when the partner URL, certificate, or DSP product changes so future deployments know what to document.
- **Tier Escalation Scheduling**
- Run `/compliance/tier-check` on a schedule (suggested: once per day or every 6h leading up to BAS lodgment). Log the cron command in your ops tracker and ensure `artifacts/compliance/tier-state/<org>.json` plus the `TIER_ESCALATION` alert exist after each run. Use the tier and forecast data to trigger downstream warnings (e.g., sending Slack/webhook notifications outside this code) before any BAS deadline slips.



============================================================
FILE: C:\src\apgms-final\scripts\apply-reword-plan.ps1
============================================================
# scripts\apply-reword-plan.ps1
param(
  [string]$Csv = "artifacts\commit_reword_suggestions.csv",
  [switch]$DryRun
)

# Resolve git.exe (avoid function-name collisions)
try {
  $script:GitExe = (Get-Command git.exe -ErrorAction Stop).Path
} catch {
  Write-Error "git not found in PATH."; exit 1
}

function Coalesce($obj, [string[]]$names) {
  foreach ($n in $names) {
    if ($obj.PSObject.Properties.Match($n).Count -gt 0) {
      $v = $obj.$n
      if ($null -ne $v -and "$v".Trim() -ne "") { return "$v" }
    }
  }
  return ""
}

function TitleCaseFirst([string]$s) {
  if ([string]::IsNullOrWhiteSpace($s)) { return $s }
  $t = $s.Trim()
  if ($t.Length -eq 0) { return $t }
  $first = $t.Substring(0,1).ToUpper()
  if ($t.Length -gt 1) { return ($first + $t.Substring(1)) }
  return $first
}

if (-not (Test-Path $Csv)) {
  Write-Error "Missing $Csv. Run the audit + generate suggestions first."
  exit 1
}

$rows = Import-Csv -Path $Csv
if (-not $rows -or $rows.Count -eq 0) {
  Write-Host "No rows in $Csv"; exit 0
}

$recs = @()
foreach ($r in $rows) {
  $hash = Coalesce $r @("hash","Hash")
  $short = Coalesce $r @("short","Short")
  $cur = Coalesce $r @("current_subject","Current_Subject","subject","Subject")
  $sugg = Coalesce $r @("suggested_subject","Suggested_Subject")
  $flags = Coalesce $r @("flags","Flags")

  if ($hash -eq "" -or $sugg -eq "") { continue }

  $hash = $hash.Trim()
  if ($short -ne "") { $short = $short.Trim() } else { $short = $hash.Substring(0,[Math]::Min(7,$hash.Length)) }
  $sugg = TitleCaseFirst($sugg.Trim())
  if ($sugg -match '\.$') { $sugg = $sugg -replace '\.$','' }

  $recs += [pscustomobject]@{
    hash = $hash
    short = $short
    current_subject = $cur
    suggested_subject = $sugg
    flags = $flags
  }
}

if ($recs.Count -eq 0) {
  Write-Host "No valid suggestions found."; exit 0
}

$outDir = "artifacts"
New-Item -ItemType Directory -Force -Path $outDir | Out-Null
$planTxt = Join-Path $outDir "commit_reword_plan.txt"
$todoTxt = Join-Path $outDir "rebase_todo_example.txt"

# Pretty plan
"" | Out-File $planTxt -Encoding utf8
foreach ($x in ($recs | Sort-Object short)) {
  Add-Content -Path $planTxt -Value ("{0} - {1}" -f $x.short, $x.suggested_subject)
  if ($x.flags)          { Add-Content -Path $planTxt -Value ("    flags: {0}" -f $x.flags) }
  if ($x.current_subject){ Add-Content -Path $planTxt -Value ("    was : {0}" -f $x.current_subject) }
}

# Rebase-todo example (template only)
"" | Out-File $todoTxt -Encoding utf8
foreach ($x in ($recs | Sort-Object short)) {
  Add-Content -Path $todoTxt -Value ("reword {0} {1}" -f $x.short, $x.suggested_subject)
}

# Console preview
Write-Host ""
Write-Host "Commits to reword (hash - new subject):"
foreach ($x in ($recs | Sort-Object short)) {
  Write-Host ("{0} - {1}" -f $x.short, $x.suggested_subject)
}

Write-Host ""
Write-Host "Wrote $planTxt"
Write-Host "Wrote $todoTxt"

if ($DryRun) {
  Write-Host ""
  Write-Host "(DRY RUN) No history changed."
  exit 0
}

Write-Host ""
Write-Host "Safety first: this script does not rewrite history."
Write-Host "Use interactive rebase to apply the rewording with Notepad."



============================================================
FILE: C:\src\apgms-final\scripts\check-coverage.mjs
============================================================
import { readFile } from 'node:fs/promises';
import { resolve } from 'node:path';

const COVERAGE_FILE = resolve(process.cwd(), 'coverage', 'coverage-summary.json');
const THRESHOLD = Number(process.env.COVERAGE_THRESHOLD ?? '85');

function fail(message) {
  console.error(message);
  process.exit(1);
}

async function main() {
  let contents;
  try {
    contents = await readFile(COVERAGE_FILE, 'utf8');
  } catch (error) {
    fail(`Unable to read coverage file at ${COVERAGE_FILE}: ${error.message}`);
  }

  let parsed;
  try {
    parsed = JSON.parse(contents);
  } catch (error) {
    fail(`coverage-summary.json is invalid JSON: ${error.message}`);
  }

  const total = parsed?.total;
  if (!total) {
    fail('coverage-summary.json missing total block');
  }

  const candidates = [];
  for (const key of ['lines', 'statements']) {
    const pct = Number(total[key]?.pct);
    if (!Number.isFinite(pct)) continue;
    candidates.push({ label: key, pct });
  }

  if (candidates.length === 0) {
    fail('coverage-summary.json missing lines/statements coverage percentages');
  }

  const lowest = candidates.reduce((acc, current) => (current.pct < acc.pct ? current : acc));
  console.log(`Coverage low-watermark: ${lowest.label} ${lowest.pct.toFixed(2)}% (threshold ${THRESHOLD}%)`);

  if (lowest.pct < THRESHOLD) {
    fail(`Coverage ${lowest.label} ${lowest.pct.toFixed(2)}% is below threshold ${THRESHOLD}%`);
  }
}

main().catch((error) => {
  console.error('coverage gate failed', error);
  process.exit(1);
});



============================================================
FILE: C:\src\apgms-final\scripts\collect-evidence.mjs
============================================================
#!/usr/bin/env node
import { spawnSync } from "node:child_process";
import { mkdirSync, writeFileSync } from "node:fs";
import { resolve } from "node:path";

const args = process.argv.slice(2);
let tag = `run-${new Date().toISOString().replace(/[:.]/g, "-")}`;
for (let i = 0; i < args.length; i++) {
  if (args[i] === "--tag" && args[i + 1]) {
    tag = args[i + 1];
    break;
  }
}

const baseUrl = process.env.EVIDENCE_BASE_URL ?? "http://localhost:3000";
const skipK6 = process.env.SKIP_K6 === "true";

const commands = [
  {
    title: "pnpm -r test",
    cmd: ["pnpm", "-r", "test"],
  },
  {
    title: "pnpm -r typecheck",
    cmd: ["pnpm", "-r", "typecheck"],
  },
  {
    title: "pnpm --filter @apgms/shared exec prisma migrate status --schema prisma/schema.prisma",
    cmd: [
      "pnpm",
      "--filter",
      "@apgms/shared",
      "exec",
      "prisma",
      "migrate",
      "status",
      "--schema",
      "prisma/schema.prisma",
    ],
  },
];

if (!skipK6) {
  commands.push({
    title: `pnpm k6:smoke -- --env BASE_URL=${baseUrl}`,
    cmd: ["pnpm", "k6:smoke", "--", "--env", `BASE_URL=${baseUrl}`],
  });
}

const evidenceDir = resolve("artifacts", "compliance");
mkdirSync(evidenceDir, { recursive: true });

let report = `# Compliance Evidence\n\n`;
report += `- Timestamp: ${new Date().toISOString()}\n`;
report += `- Git commit: ${runCommand("git", ["rev-parse", "HEAD"]).stdout.trim()}\n`;
report += `- Tag: ${tag}\n`;
report += `- Base URL: ${baseUrl}\n`;
report += `- Skip k6: ${skipK6}\n\n`;

for (const { title, cmd } of commands) {
  report += `## ${title}\n\n`;
  const { stdout, stderr, status } = runCommand(cmd[0], cmd.slice(1));
  report += `- Exit code: ${status}\n\n`;
  if (stdout.trim().length > 0) {
    report += "```text\n";
    report += stdout.trim();
    report += "\n```\n\n";
  } else {
    report += "_No stdout output_\n\n";
  }
  if (stderr.trim().length > 0) {
    report += "```text\n";
    report += stderr.trim();
    report += "\n```\n\n";
  }
}

const filePath = resolve(evidenceDir, `${tag}.md`);
writeFileSync(filePath, report, "utf8");
console.log(`Evidence written to ${filePath}`);

function runCommand(command, args) {
  const result = spawnSync(command, args, { encoding: "utf8", shell: process.platform === "win32" });
  return {
    stdout: result.stdout ?? "",
    stderr: result.stderr ?? "",
    status: typeof result.status === "number" ? result.status : 1,
  };
}



============================================================
FILE: C:\src\apgms-final\scripts\collect-monitoring-evidence.mjs
============================================================
import { mkdir, writeFile } from "node:fs/promises";
import { join } from "node:path";

const baseUrl = process.env.BASE_URL ?? "http://localhost:3000";
const token = process.env.APIGMS_MONITORING_TOKEN;

if (!token) {
  throw new Error("APIGMS_MONITORING_TOKEN is required");
}

const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
const outputDir = join("artifacts", "monitoring", timestamp);
await mkdir(outputDir, { recursive: true });

const endpoints = [
  { path: "/monitor/compliance", file: "compliance.json" },
  { path: "/monitor/risk", file: "risk.json" },
];

for (const endpoint of endpoints) {
  const response = await fetch(`${baseUrl}${endpoint.path}`, {
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${token}`,
    },
  });
  const body = await response.text();
  await writeFile(join(outputDir, endpoint.file), body, "utf-8");
}

console.info(`Saved monitoring evidence to ${outputDir}`);



============================================================
FILE: C:\src\apgms-final\scripts\db-reset.ps1
============================================================
# db-reset script



============================================================
FILE: C:\src\apgms-final\scripts\dev-up.ps1
============================================================
# dev-up script



============================================================
FILE: C:\src\apgms-final\scripts\e2e-run.ps1
============================================================
# e2e-run script



============================================================
FILE: C:\src\apgms-final\scripts\Export-CodeDumpPdf.ps1
============================================================
# Export-CodeDumpPdf.ps1
# Scans the folder this script is in (recursively), collects source/text files,
# and generates one or more PDFs of ~code dump back-to-back.
#
# Each PDF is capped at MaxPages pages. If you need more than MaxPages,
# it will automatically create CodeDump_part01.pdf, CodeDump_part02.pdf, etc.
#
# Usage:
#   powershell -ExecutionPolicy Bypass -File .\Export-CodeDumpPdf.ps1

param(
    [int]$MaxPages = 100,          # max pages per PDF file
    [int]$LinesPerPage = 75,       # ~75 lines per page at 8pt font
    [int]$MaxCols = 100,           # wrap code lines to this width
    [string]$BaseName = "CodeDump" # base filename, we'll append _partXX if needed
)

# -------------------------
# Helpers
# -------------------------

function Wrap-Line {
    param(
        [string]$Text,
        [int]$MaxCols
    )
    # Return list of lines wrapped to MaxCols columns.
    $result = @()
    if ($null -eq $Text) { return @("") }
    if ($Text.Length -eq 0) { return @("") }

    $start = 0
    while ($start -lt $Text.Length) {
        $len = [Math]::Min($MaxCols, $Text.Length - $start)
        $segment = $Text.Substring($start, $len)
        $result += $segment
        $start += $len
    }
    return ,$result
}

function Escape-PdfText {
    param(
        [string]$Line
    )
    # Make line safe for a PDF text object.
    # We also coerce to ASCII (non-ASCII => "?") so the minimal font works.
    if ($null -eq $Line) { $Line = "" }

    $bytes = [System.Text.Encoding]::ASCII.GetBytes($Line)
    $clean = [System.Text.Encoding]::ASCII.GetString($bytes)

    # Escape backslash, "(" and ")"
    $clean = $clean.Replace("\", "\\")
    $clean = $clean.Replace("(", "\(")
    $clean = $clean.Replace(")", "\)")

    return $clean
}

function Build-Pdf {
    param(
        [System.Collections.Generic.List[object]]$Pages, # list of string[] pageLines
        [string]$OutFile
    )

    # We emit a minimal valid PDF 1.4 with:
    # 1 0 obj -> Catalog
    # 2 0 obj -> Pages
    # 3 0 obj -> Font (Courier)
    # Then for each page:
    #   Page obj
    #   Content stream obj

    $numPages = $Pages.Count

    $pageObjects     = @()
    $contentObjects  = @()
    $kidsRefs        = @()

    for ($i = 0; $i -lt $numPages; $i++) {
        $pageObjNum    = 4 + (2 * $i)
        $contentObjNum = 5 + (2 * $i)

        $kidsRefs += ("{0} 0 R" -f $pageObjNum)

        # Build the PDF text stream for this page
        $sbPage = New-Object System.Text.StringBuilder
        [void]$sbPage.Append("BT`n")
        [void]$sbPage.Append("/F1 8 Tf`n")     # Courier 8pt
        [void]$sbPage.Append("9.5 TL`n")       # line spacing
        [void]$sbPage.Append("36 756 Td`n")    # start near top-left margin

        $pageLines = [string[]]$Pages[$i]
        foreach ($rawLine in $pageLines) {
            $esc = Escape-PdfText -Line $rawLine
            [void]$sbPage.Append("(" + $esc + ") Tj`nT*`n")
        }

        [void]$sbPage.Append("ET`n")

        $streamBody       = $sbPage.ToString()
        $streamBodyBytes  = [System.Text.Encoding]::ASCII.GetBytes($streamBody)
        $lenBytes         = $streamBodyBytes.Length

        $contentObjString = ("<< /Length {0} >>`nstream`n{1}endstream" -f $lenBytes, $streamBody)

        $pageObjString = "<< /Type /Page /Parent 2 0 R /MediaBox [0 0 612 792] /Contents {0} 0 R /Resources << /Font << /F1 3 0 R >> >> >>" -f $contentObjNum

        $contentObjects += @{
            Num  = $contentObjNum
            Data = $contentObjString
        }
        $pageObjects += @{
            Num  = $pageObjNum
            Data = $pageObjString
        }
    }

    $kidsArray = "[ " + ($kidsRefs -join " ") + " ]"

    $obj1 = @{
        Num  = 1
        Data = "<< /Type /Catalog /Pages 2 0 R >>"
    }

    $obj2 = @{
        Num  = 2
        Data = "<< /Type /Pages /Kids " + $kidsArray + " /Count " + $numPages + " >>"
    }

    $obj3 = @{
        Num  = 3
        Data = "<< /Type /Font /Subtype /Type1 /BaseFont /Courier >>"
    }

    $allObjs = @($obj1, $obj2, $obj3) + ($pageObjects + $contentObjects | Sort-Object Num)
    $allObjs = $allObjs | Sort-Object Num

    $objectStrings = @()
    foreach ($o in $allObjs) {
        $objectStrings += ("{0} 0 obj`n{1}`nendobj`n" -f $o.Num, $o.Data)
    }

    $pdfHeader = "%PDF-1.4`n"
    $enc       = [System.Text.Encoding]::ASCII

    # compute xref offsets
    $offsets = @()
    $pos = $enc.GetByteCount($pdfHeader)

    foreach ($objStr in $objectStrings) {
        $offsets += $pos
        $pos += $enc.GetByteCount($objStr)
    }

    $startXref = $pos
    $totalObjects = $allObjs.Count + 1  # include object 0

    # xref table
    $xrefSb = New-Object System.Text.StringBuilder
    [void]$xrefSb.Append("xref`n")
    [void]$xrefSb.Append("0 $totalObjects`n")
    [void]$xrefSb.Append("0000000000 65535 f `n")
    foreach ($off in $offsets) {
        $offStr = $off.ToString("0000000000")
        [void]$xrefSb.Append("$offStr 00000 n `n")
    }

    $xrefSection = $xrefSb.ToString()

    # trailer
    $trailerSection = "trailer`n<< /Size $totalObjects /Root 1 0 R >>`nstartxref`n$startXref`n%%EOF`n"

    $pdfString = $pdfHeader + ($objectStrings -join "") + $xrefSection + $trailerSection
    $pdfBytes  = $enc.GetBytes($pdfString)

    [System.IO.File]::WriteAllBytes((Join-Path (Get-Location) $OutFile), $pdfBytes)
}

# -------------------------
# MAIN
# -------------------------

Write-Host "[1/4] Collecting files..."

$baseDir = Split-Path -Parent $MyInvocation.MyCommand.Path
if ([string]::IsNullOrWhiteSpace($baseDir)) {
    $baseDir = Get-Location
}

# skip obvious binaries so we don't blast garbage into the PDF
$binaryExtPattern = '^\.(exe|dll|so|bin|png|jpg|jpeg|gif|ico|pdf|zip|7z|gz|tar|mp4|mp3|wav|ogg|avi|mov|class|jar|ttf|otf|woff|woff2|psd|xls|xlsx|doc|docx|ppt|pptx)$'

$files = Get-ChildItem -Path $baseDir -Recurse -File | Where-Object {
    $_.Extension -notmatch $binaryExtPattern
}

Write-Host "[2/4] Reading and wrapping file contents..."

# Collect all wrapped lines from all files into one giant list
$allLines = New-Object System.Collections.Generic.List[string]

foreach ($f in $files) {
    $relPath = Resolve-Path -Relative $f.FullName

    $allLines.Add("===== BEGIN FILE: $relPath =====")

    $rawText = Get-Content -LiteralPath $f.FullName -Raw -ErrorAction SilentlyContinue
    if ($null -eq $rawText) { $rawText = "" }

    $rawLines = $rawText -split "(`r`n|`n|`r)"

    foreach ($ln in $rawLines) {
        $wrappedLines = Wrap-Line -Text $ln -MaxCols $MaxCols
        foreach ($w in $wrappedLines) {
            $allLines.Add($w)
        }
    }

    $allLines.Add("") # spacer after file
}

Write-Host "[3/4] Paginating ALL pages (no cap yet)..."

# Turn that big line list into a list of pages,
# where each page is an array of up to $LinesPerPage lines.
$allPages = New-Object System.Collections.Generic.List[object]
$currentPage = New-Object System.Collections.Generic.List[string]

foreach ($line in $allLines) {
    if ($currentPage.Count -ge $LinesPerPage) {
        $allPages.Add($currentPage.ToArray())
        $currentPage = New-Object System.Collections.Generic.List[string]
    }
    $currentPage.Add($line)
}

# add last partially-filled page
if ($currentPage.Count -gt 0) {
    $allPages.Add($currentPage.ToArray())
}

$totalPages = $allPages.Count
Write-Host ("    -> Total logical pages: {0}" -f $totalPages)

if ($totalPages -eq 0) {
    # edge case: nothing to print at all
    $emptyPage = @("<< no printable text found >>")
    $allPages.Add($emptyPage)
    $totalPages = 1
}

Write-Host "[4/4] Building PDF file(s)..."

# Now chunk pages into batches of up to $MaxPages each.
# Example: if totalPages=230 and MaxPages=100:
#   batch 1 -> pages 0..99
#   batch 2 -> pages 100..199
#   batch 3 -> pages 200..229
#
# We'll name:
#   CodeDump.pdf            (if only 1 batch)
#   CodeDump_part01.pdf     (first batch)
#   CodeDump_part02.pdf     (second batch)
#   ...

$batchCount = [Math]::Ceiling($totalPages / $MaxPages)
Write-Host ("    -> Will generate {0} PDF file(s)." -f $batchCount)

for ($b = 0; $b -lt $batchCount; $b++) {
    $startPage = $b * $MaxPages
    $endPage   = [Math]::Min($startPage + $MaxPages, $totalPages) - 1

    $subset = New-Object System.Collections.Generic.List[object]
    for ($p = $startPage; $p -le $endPage; $p++) {
        $subset.Add([string[]]$allPages[$p])
    }

    if ($batchCount -eq 1) {
        # single file case
        $outName = "$BaseName.pdf"
    } else {
        $partNum = ($b+1).ToString("00")
        $outName = "{0}_part{1}.pdf" -f $BaseName, $partNum
    }

    Write-Host ("    -> Writing pages {0} to {1} into {2}" -f ($startPage+1), ($endPage+1), $outName)

    Build-Pdf -Pages $subset -OutFile $outName
}

Write-Host "Done."
Write-Host ("Generated {0} PDF file(s) in {1}" -f $batchCount, (Get-Location).Path)
Write-Host "Each file holds up to $MaxPages pages of code."



============================================================
FILE: C:\src\apgms-final\scripts\export-dump.ps1
============================================================
# export-dump script



============================================================
FILE: C:\src\apgms-final\scripts\export-evidence-pack.ts
============================================================
import { mkdir, writeFile } from "node:fs/promises";
import { resolve, join } from "node:path";

type CliOptions = {
  baseUrl: string;
  token: string;
  orgId: string;
  outDir: string;
};

function parseArgs(): CliOptions {
  const defaults = {
    baseUrl: process.env.APGMS_API_URL ?? "http://localhost:3000",
    orgId: process.env.APGMS_ORG_ID ?? "dev-org",
    outDir: process.env.APGMS_EXPORT_DIR ?? "artifacts/backups",
    token: process.env.APGMS_API_TOKEN ?? "",
  };

  const args = process.argv.slice(2);
  for (let i = 0; i < args.length; i += 1) {
    const key = args[i];
    const value = args[i + 1];
    if (!value) continue;
    switch (key) {
      case "--base-url":
        defaults.baseUrl = value;
        i += 1;
        break;
      case "--token":
        defaults.token = value;
        i += 1;
        break;
      case "--org":
        defaults.orgId = value;
        i += 1;
        break;
      case "--out":
        defaults.outDir = value;
        i += 1;
        break;
      default:
        break;
    }
  }

  if (!defaults.token) {
    throw new Error("API token missing. Provide --token argument or set APGMS_API_TOKEN.");
  }

  return defaults;
}

async function fetchJson<T>(baseUrl: string, path: string, token: string): Promise<T> {
  const target = `${baseUrl.replace(/\/$/, "")}${path}`;
  const response = await fetch(target, {
    headers: {
      Authorization: `Bearer ${token}`,
      "Content-Type": "application/json",
    },
  });

  if (!response.ok) {
    const text = await response.text();
    throw new Error(`Request to ${path} failed (${response.status}): ${text || "no response body"}`);
  }

  return response.json() as Promise<T>;
}

async function postJson<T>(
  baseUrl: string,
  path: string,
  token: string,
  body: unknown,
): Promise<T> {
  const target = `${baseUrl.replace(/\/$/, "")}${path}`;
  const response = await fetch(target, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${token}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify(body ?? {}),
  });

  if (!response.ok) {
    const text = await response.text();
    throw new Error(`POST to ${path} failed (${response.status}): ${text || "no response body"}`);
  }

  return response.json() as Promise<T>;
}

async function main() {
  const options = parseArgs();
  const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
  const outDir = resolve(process.cwd(), options.outDir);
  const targetFile = join(outDir, `evidence-pack_${options.orgId}_${timestamp}.json`);

  await mkdir(outDir, { recursive: true });

  const [orgExport, complianceReport] = await Promise.all([
    fetchJson<unknown>(
      options.baseUrl,
      `/admin/export/${encodeURIComponent(options.orgId)}`,
      options.token,
    ),
    fetchJson<unknown>(options.baseUrl, "/compliance/report", options.token),
  ]);

  const createdArtifact = await postJson<{ artifact: { id: string } }>(
    options.baseUrl,
    "/compliance/evidence",
    options.token,
    {},
  );

  const detailedArtifact = await fetchJson<{
    artifact: {
      id: string;
      sha256: string;
      createdAt: string;
      kind: string;
      payload: unknown;
    };
  }>(options.baseUrl, `/compliance/evidence/${createdArtifact.artifact.id}`, options.token);

  const payload = {
    generatedAt: new Date().toISOString(),
    orgId: options.orgId,
    inputs: {
      baseUrl: options.baseUrl,
    },
    export: orgExport,
    compliance: complianceReport,
    evidenceArtifact: detailedArtifact.artifact,
  };

  await writeFile(targetFile, `${JSON.stringify(payload, null, 2)}\n`, "utf8");

  // eslint-disable-next-line no-console
  console.log(`âœ… Evidence pack written to ${targetFile}`);
}

main().catch((error) => {
  // eslint-disable-next-line no-console
  console.error("Failed to generate evidence pack:", error);
  process.exitCode = 1;
});



============================================================
FILE: C:\src\apgms-final\scripts\Export-FolderTree.ps1
============================================================
# Export-FolderTree.ps1
# Generates a tree of the folder this script is in and writes it to folder-structure.txt

# 1. Get the directory where this script lives
$BasePath = Split-Path -Parent $MyInvocation.MyCommand.Path

# 2. Where to write the output
$OutFile = Join-Path $BasePath "folder-structure.txt"

Write-Host "Scanning $BasePath ..."
Write-Host "Writing to $OutFile ..."

# 3. Get all dirs/files under that base path
$items = Get-ChildItem -LiteralPath $BasePath -Recurse |
    Sort-Object FullName

# 4. Build a relative "tree" view
$result = foreach ($item in $items) {
    # turn C:\src\whatever\services\api-gateway\src\app.ts
    # into .\services\api-gateway\src\app.ts
    $rel = $item.FullName.Substring($BasePath.Length).TrimStart('\','/')
    if ($rel -eq "") { continue }

    # indent based on depth
    $parts = $rel -split "[\\/]"
    $indentLevel = $parts.Length - 1
    $indent = ("  " * $indentLevel) + "|- "

    $indent + $parts[-1]
}

# 5. Prepend the root folder name at the top
$header = @(
    "Root: $BasePath"
    "|"
    "|- (root)"
)

# 6. Write everything to file
$allLines = $header + $result
$allLines | Set-Content -Encoding UTF8 $OutFile

Write-Host "Done."
Write-Host "Preview:"
Write-Host "---------------------------------"
$allLines | Select-Object -First 40 | ForEach-Object { Write-Host $_ }
Write-Host "---------------------------------"
Write-Host "(Only first 40 lines shown above)"



============================================================
FILE: C:\src\apgms-final\scripts\generate-commit-reword-plan.ps1
============================================================
# scripts\generate-commit-reword-plan.ps1
# Reads artifacts\commit_audit.csv and produces:
# - artifacts\commit_reword_suggestions.csv
# - artifacts\commit_reword_plan.txt
param(
  [string]$CsvPath = "artifacts\commit_audit.csv",
  [int]$MaxSubject = 72
)

function Coalesce($obj, [string[]]$names) {
  foreach ($n in $names) {
    if ($obj.PSObject.Properties.Match($n).Count -gt 0) {
      $v = $obj.$n
      if ($null -ne $v -and "$v".Trim() -ne "") { return "$v" }
    }
  }
  return ""
}

function TitleCaseFirst([string]$s) {
  if ([string]::IsNullOrWhiteSpace($s)) { return $s }
  $t = $s.Trim()
  if ($t.Length -eq 0) { return $t }
  $first = $t.Substring(0,1).ToUpper()
  if ($t.Length -gt 1) { return ($first + $t.Substring(1)) }
  return $first
}

function BuildSubject([string]$prefix, [string]$scope, [string]$rest, [int]$maxLen) {
  $p = $prefix
  if ($scope -and $scope.Trim() -ne "") {
    if ($p -and $p -notmatch '\($') { $p = "$p($scope)" } else { $p = "$prefix($scope)" }
  }
  if ($p) { $p = "$($p): " }  # NOTE: wrap in $() to avoid $p: parsing
  $subj = "$p$rest"
  if ($subj.Length -gt $maxLen) { $subj = $subj.Substring(0, $maxLen).TrimEnd() }
  return $subj
}

function GuessPrefixFromRow($row) {
  $pfx = Coalesce $row @("prefix","Prefix")
  if ($pfx -ne "") { return $pfx.Trim() }

  $inf = (Coalesce $row @("inferred_type","Inferred_Type")).ToLower()
  switch -regex ($inf) {
    '^docs$'          { return 'docs' }
    '^test$'          { return 'test' }
    '^feat\+test$'    { return 'feat' }
    '^feat\(db\)$'    { return 'feat' }
    '^chore\(db\)$'   { return 'chore' }
    '^chore\(deps\)$' { return 'chore' }
    '^ci$'            { return 'ci' }
    '^refactor$'      { return 'refactor' }
    '^feat$'          { return 'feat' }
    default           { return 'chore' }
  }
}

function GuessScopeFromRow($row) {
  $sc = Coalesce $row @("scope","Scope")
  if ($sc -ne "") { return $sc.Trim() }
  $dir = (Coalesce $row @("dominant_dir","Dominant_Dir")).Trim()
  if ($dir -and $dir -ne "." -and $dir -ne "docs") { return $dir }
  return ""
}

function NeedsRewrite([string]$flagsList) {
  if ($null -eq $flagsList) { $flagsList = "" }
  $needles = @(
    'subject-not-capitalized',
    'subject>72',
    'docs-claimed-mismatch',
    'test-claimed-mismatch',
    'deps-looks-like-chore',
    'code-additions-look-like-feat',
    'code-changes-look-like-refactor',
    'scope-mismatch',
    'needs-body(>=150 LOC)',
    'possible-breaking-missing-marker'
  )
  foreach ($n in $needles) {
    if ($flagsList -match [regex]::Escape($n)) { return $true }
  }
  return $false
}

if (-not (Test-Path $CsvPath)) {
  Write-Error "Missing $CsvPath. Run your audit script first."
  exit 1
}

$df = Import-Csv -Path $CsvPath
if (-not $df -or $df.Count -eq 0) {
  Write-Host "No rows in $CsvPath"
  exit 0
}

$recs = @()
foreach ($row in $df) {
  $flagsText = Coalesce $row @("flags","Flags")
  if (-not (NeedsRewrite $flagsText)) { continue }

  $prefix = GuessPrefixFromRow $row
  $scope  = GuessScopeFromRow  $row

  $rest = Coalesce $row @("subject","Subject")
  if ($rest -match '^[a-z]+(\([^)]+\))?:\s*(?<r>.*)$') { $rest = $Matches['r'] }
  $rest = TitleCaseFirst($rest)
  if ($rest -match '\.$') { $rest = $rest -replace '\.$','' }

  if ($flagsText -match 'code-additions-look-like-feat' -and $prefix -notmatch '^(feat|fix)') {
    $prefix = 'feat'
  }
  if ($flagsText -match 'code-changes-look-like-refactor' -and $prefix -notmatch '^(refactor|chore)') {
    $prefix = 'refactor'
  }
  if ($flagsText -match 'deps-looks-like-chore' -and $prefix -notmatch '^chore') {
    $prefix = 'chore'
  }

  $suggestedSubject = BuildSubject $prefix $scope $rest $MaxSubject

  $recs += [pscustomobject]@{
    hash              = Coalesce $row @("hash","Hash")
    short             = Coalesce $row @("short","Short")
    current_subject   = Coalesce $row @("subject","Subject")
    suggested_subject = $suggestedSubject
    flags             = $flagsText
  }
}

$outDir = "artifacts"
New-Item -ItemType Directory -Force -Path $outDir | Out-Null

$csvOut = Join-Path $outDir "commit_reword_suggestions.csv"
$txtOut = Join-Path $outDir "commit_reword_plan.txt"

$recs | Sort-Object -Property hash | Export-Csv -NoTypeInformation -Encoding UTF8 -Path $csvOut

"" | Out-File $txtOut -Encoding utf8
foreach ($r in ($recs | Sort-Object -Property short)) {
  Add-Content -Path $txtOut -Value ("{0} - {1}" -f $r.short, $r.suggested_subject)
  if ($r.flags) { Add-Content -Path $txtOut -Value ("    flags: {0}" -f $r.flags) }
  if ($r.current_subject) { Add-Content -Path $txtOut -Value ("    was : {0}" -f $r.current_subject) }
}

Write-Host "Wrote $csvOut"
Write-Host "Wrote $txtOut"



============================================================
FILE: C:\src\apgms-final\scripts\k6-load.ps1
============================================================
# k6-load script



============================================================
FILE: C:\src\apgms-final\scripts\key-rotate.ps1
============================================================
# key-rotate script



============================================================
FILE: C:\src\apgms-final\scripts\regulator-smoke.mjs
============================================================
#!/usr/bin/env node
import process from "node:process";

const API_BASE_URL = process.env.API_BASE_URL ?? "http://localhost:3000";
const ACCESS_CODE = process.env.REGULATOR_ACCESS_CODE ?? "regulator-dev-code";
const ORG_ID = process.env.REGULATOR_ORG_ID ?? "dev-org";

async function request(path, options = {}) {
  const url = `${API_BASE_URL}${path}`;
  const res = await fetch(url, options);
  if (!res.ok) {
    const body = await res.text().catch(() => "");
    throw new Error(`Request failed: ${options.method ?? "GET"} ${path} -> ${res.status} ${body}`);
  }
  const contentType = res.headers.get("content-type") ?? "";
  if (contentType.includes("application/json")) {
    return res.json();
  }
  return res.text();
}

async function main() {
  console.log(`[info] Regulator smoke against ${API_BASE_URL} (org: ${ORG_ID})`);

  const login = await request("/regulator/login", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      accessCode: ACCESS_CODE,
      orgId: ORG_ID,
    }),
  });

  const token = login.token;
  if (!token) {
    throw new Error("Login response missing token");
  }

  const authHeaders = {
    Authorization: `Bearer ${token}`,
    "Content-Type": "application/json",
  };

  await request("/regulator/health");
  console.log("  [ok] health endpoint reachable");

  const compliance = await request("/regulator/compliance/report", { headers: authHeaders });
  console.log(
    `  [ok] compliance report fetched (${compliance.basHistory?.length ?? 0} BAS periods, ` +
      `${compliance.alertsSummary?.openHighSeverity ?? 0} high alerts open)`
  );

  const evidence = await request("/regulator/evidence", { headers: authHeaders });
  console.log(`  [ok] evidence catalogue returned (${evidence.artifacts?.length ?? 0} artifacts)`);

  const snapshots = await request("/regulator/monitoring/snapshots?limit=1", {
    headers: authHeaders,
  });
  console.log(
    `  [ok] monitoring snapshot access ok (latest id: ${
      snapshots.snapshots?.[0]?.id ?? "none"
    })`
  );

  const bankSummary = await request("/regulator/bank-lines/summary", {
    headers: authHeaders,
  });
  console.log(
    `  [ok] bank summary totals AUD ${Number(bankSummary.summary?.totalAmount ?? 0).toFixed(2)}`
  );

  console.log("[done] Regulator smoke completed");
}

main().catch((error) => {
  console.error("[fail] Regulator smoke failed");
  console.error(error);
  process.exit(1);
});



============================================================
FILE: C:\src\apgms-final\scripts\rotate-pii-keys.mjs
============================================================
#!/usr/bin/env node
import { writeFileSync, readFileSync } from "node:fs";
import { resolve } from "node:path";
import { randomBytes, generateKeyPairSync } from "node:crypto";

const args = process.argv.slice(2);
let envPath = null;
let dryRun = true;

for (let i = 0; i < args.length; i++) {
  if (args[i] === "--write-env" && args[i + 1]) {
    envPath = resolve(args[i + 1]);
    dryRun = false;
  }
}

const piiKeyMaterial = randomBytes(32).toString("base64");
const piiSaltMaterial = randomBytes(32).toString("base64");
const activeKid = `kid-${Date.now()}`;
const activeSid = `sid-${Date.now()}`;

const { publicKey, privateKey } = generateKeyPairSync("rsa", {
  modulusLength: 2048,
});

const publicJwk = publicKey.export({ format: "jwk" });
publicJwk.alg = "RS256";
publicJwk.use = "sig";
publicJwk.kid = activeKid;

const envUpdates = {
  AUTH_JWKS: JSON.stringify({ keys: [publicJwk] }),
  PII_KEYS: JSON.stringify([{ kid: activeKid, material: piiKeyMaterial }]),
  PII_ACTIVE_KEY: activeKid,
  PII_SALTS: JSON.stringify([{ sid: activeSid, secret: piiSaltMaterial }]),
  PII_ACTIVE_SALT: activeSid,
};

console.log("# Generated secrets");
console.log(JSON.stringify(envUpdates, null, 2));
console.log("\n# Private key (store securely)");
console.log(privateKey.export({ format: "pem", type: "pkcs8" }).toString());

if (dryRun) {
  console.log("\nNo changes written. Pass --write-env <path> to update an env file.");
  process.exit(0);
}

const envFile = envPath ?? resolve(".env");
let existing = "";
try {
  existing = readFileSync(envFile, "utf8");
} catch {
  // New file; leave existing empty
}

let updated = existing;
for (const [key, value] of Object.entries(envUpdates)) {
  const pattern = new RegExp(`^${escapeRegex(key)}=.*$`, "m");
  const line = `${key}=${value}`;
  if (pattern.test(updated)) {
    updated = updated.replace(pattern, line);
  } else {
    if (updated.length && !updated.endsWith("\n")) {
      updated += "\n";
    }
    updated += `${line}\n`;
  }
}

writeFileSync(envFile, updated, "utf8");
console.log(`Updated ${envFile} with new key material.`);

function escapeRegex(value) {
  return value.replace(/[.*+?^${}()|[\]\\]/g, "\\$&");
}



============================================================
FILE: C:\src\apgms-final\scripts\run-webapp-tests.ps1
============================================================
param(
  [int]$Port = 5173,
  [string]$BaseUrl,
  [switch]$StartApi
)

$ErrorActionPreference = 'Stop'

# --- Paths ---
$repo = (Resolve-Path ".").Path
$web  = Join-Path $repo "webapp"
$api  = Join-Path $repo "services\api-gateway"
$configPath = Join-Path $web "playwright.config.ts"
$pkgPath    = Join-Path $web "package.json"

if (-not (Test-Path $web)) { throw "Can't find webapp folder at $web" }
if (-not (Test-Path $pkgPath)) { throw "Can't find $pkgPath" }

# --- Env for Playwright/Vite ---
if (-not $BaseUrl -or [string]::IsNullOrWhiteSpace($BaseUrl)) {
  $BaseUrl = "http://localhost:$Port"
}
$env:WEBAPP_PORT = "$Port"
$env:WEBAPP_BASE_URL = $BaseUrl

Write-Host ("WEBAPP_PORT=" + $env:WEBAPP_PORT)
Write-Host ("WEBAPP_BASE_URL=" + $env:WEBAPP_BASE_URL)

# --- Helpers ---
function Ensure-Script {
  param([object]$Pkg,[string]$Name,[string]$Value)
  if (-not $Pkg.scripts.PSObject.Properties[$Name]) {
    $Pkg.scripts | Add-Member -MemberType NoteProperty -Name $Name -Value $Value
  } elseif (-not $Pkg.scripts.$Name -or [string]::IsNullOrWhiteSpace([string]$Pkg.scripts.$Name)) {
    $Pkg.scripts.$Name = $Value
  }
}

function Remove-Bom {
  param([string]$Path)
  if (-not (Test-Path $Path)) { return }
  $bytes = [System.IO.File]::ReadAllBytes($Path)
  if ($bytes.Length -ge 3 -and $bytes[0] -eq 239 -and $bytes[1] -eq 187 -and $bytes[2] -eq 191) {
    [System.IO.File]::WriteAllBytes($Path, $bytes[3..($bytes.Length-1)])
    Write-Host ("Stripped BOM from " + $Path)
  }
}

function ShellExe { if (Get-Command pwsh -ErrorAction SilentlyContinue) { 'pwsh' } else { 'powershell' } }

function Get-HealthOk {
  try {
    $r = Invoke-WebRequest http://localhost:3000/health -TimeoutSec 3
    return ($r.StatusCode -eq 200)
  } catch { return $false }
}

# --- 1) Write playwright.config.ts ---
$configLines = @(
"import { defineConfig, devices } from '@playwright/test';",
"" ,
"const PORT = Number(process.env.WEBAPP_PORT || 5173);",
"const BASE_URL = process.env.WEBAPP_BASE_URL || `http://localhost:${PORT}`;",
"" ,
"export default defineConfig({",
"  testDir: './tests',",
"  use: {",
"    baseURL: BASE_URL,",
"    trace: 'on-first-retry',",
"  },",
"  webServer: [",
"    {",
"      command: process.env.CI ? `pnpm preview --port ${PORT}` : `pnpm dev --port ${PORT}`,",
"      url: BASE_URL,",
"      reuseExistingServer: !process.env.CI,",
"      timeout: 120000,",
"      stdout: 'pipe',",
"      stderr: 'pipe',",
"    },",
"  ],",
"  projects: [",
"    { name: 'chromium', use: { ...devices['Desktop Chrome'] } },",
"  ],",
"});"
)
Set-Content -Path $configPath -Value $configLines -Encoding UTF8
Write-Host ("Wrote " + $configPath)

# --- 2) Ensure package.json has scripts ---
$pkgJson = Get-Content $pkgPath -Raw | ConvertFrom-Json
if (-not $pkgJson.PSObject.Properties['scripts']) {
  $scriptsObj = New-Object psobject
  $pkgJson | Add-Member -MemberType NoteProperty -Name scripts -Value $scriptsObj
}
Ensure-Script $pkgJson 'dev'     'vite'
Ensure-Script $pkgJson 'build'   'vite build'
Ensure-Script $pkgJson 'preview' 'vite preview'
($pkgJson | ConvertTo-Json -Depth 100) | Set-Content -Path $pkgPath -Encoding UTF8
Write-Host ("Updated scripts in " + $pkgPath)

# --- 3) Strip BOMs from common config files ---
$maybeBom = @(
  (Join-Path $web 'postcss.config.json'),
  (Join-Path $web 'postcss.config.cjs'),
  (Join-Path $web 'postcss.config.js'),
  (Join-Path $web 'tsconfig.json'),
  (Join-Path $web 'vite.config.ts'),
  (Join-Path $web 'vite.config.js')
) | Where-Object { $_ -ne $null }
foreach ($p in $maybeBom) { Remove-Bom -Path $p }

# --- 4) Install deps + Playwright browsers ---
Push-Location $repo
try {
  Write-Host "Enabling corepack..."
  try { corepack enable | Out-Null } catch { Write-Warning ("corepack enable failed (continuing): " + $_.Exception.Message) }

  Write-Host "Installing workspace deps (pnpm install --frozen-lockfile)..."
  pnpm install --frozen-lockfile

  Write-Host "Approving postinstall builds (prisma engines, esbuild)..."
  pnpm approve-builds prisma @prisma/engines esbuild

  Write-Host "Installing Playwright browsers (and OS deps if supported)..."
  pnpm exec playwright install --with-deps
}
finally { Pop-Location }

# --- 5) Start API (only if not already healthy) ---
$apiProc = $null
if ($StartApi) {
  if (Get-HealthOk) {
    Write-Host "API already responding on :3000 ? not starting another."
  } else {
    Write-Host "Starting API (dev)..."
    Push-Location $api
    try {
      $apiProc = Start-Process -PassThru -NoNewWindow (ShellExe) -ArgumentList '-NoLogo','-NoProfile','-Command','pnpm dev'
      Write-Host "Waiting for http://localhost:3000/health ..."
      $deadline = (Get-Date).AddSeconds(30)
      while ((Get-Date) -lt $deadline) {
        if (Get-HealthOk) { break }
        Start-Sleep -Milliseconds 500
      }
      if (-not (Get-HealthOk)) { Write-Warning "API health did not return 200 within 30s; tests may fail." }
    } finally {
      Pop-Location
    }
  }
}

# --- 6) Build then Test ---
Push-Location $repo
try {
  Write-Host ""
  Write-Host "Building packages (pnpm -r build)..."
  pnpm -r build

  Write-Host ""
  Write-Host "Running tests (pnpm -r test)..."
  pnpm -r test
}
finally {
  Pop-Location
  if ($apiProc) {
    Write-Host "Stopping API..."
    try { Stop-Process -Id $apiProc.Id -Force } catch {}
  }
}

Write-Host ""
Write-Host "Done."



============================================================
FILE: C:\src\apgms-final\scripts\seed.ps1
============================================================
# seed script



============================================================
FILE: C:\src\apgms-final\scripts\seed.ts
============================================================
import { Buffer } from "node:buffer";

import { prisma } from "@apgms/shared/db";
import { hashPassword } from "@apgms/shared";
import { encryptPII, configurePIIProviders } from "../services/api-gateway/src/lib/pii";
import { createKeyManagementService, createSaltProvider } from "../services/api-gateway/src/security/providers";

async function main() {
  process.env.PII_KEYS ??= JSON.stringify([{ kid: "seed-key", material: Buffer.alloc(32, 5).toString("base64") }]);
  process.env.PII_ACTIVE_KEY ??= "seed-key";
  process.env.PII_SALTS ??= JSON.stringify([{ sid: "seed-salt", secret: Buffer.alloc(32, 6).toString("base64") }]);
  process.env.PII_ACTIVE_SALT ??= "seed-salt";

  const kms = await createKeyManagementService();
  const saltProvider = await createSaltProvider();
  configurePIIProviders({
    kms,
    saltProvider,
    auditLogger: {
      record: async () => {
        // Seeding audit logs is unnecessary; no-op.
      },
    },
  });

  const org = await prisma.org.upsert({
    where: { id: "demo-org" },
    update: {},
    create: { id: "demo-org", name: "Demo Org" },
  });

  const hashedPassword = await hashPassword("password123");

  await prisma.user.upsert({
    where: { email: "founder@example.com" },
    update: {},
    create: { email: "founder@example.com", password: hashedPassword, orgId: org.id },
  });

  const today = new Date();
  const lines = [
    { date: new Date(today.getFullYear(), today.getMonth(), today.getDate() - 2), amount: 1250.75, payee: "Acme", desc: "Office fit-out" },
    { date: new Date(today.getFullYear(), today.getMonth(), today.getDate() - 1), amount: -299.99, payee: "CloudCo", desc: "Monthly sub" },
    { date: today, amount: 5000.0, payee: "Birchal", desc: "Investment received" },
  ].map((entry) => {
    const payee = encryptPII(entry.payee);
    const desc = encryptPII(entry.desc);
    return {
      orgId: org.id,
      date: entry.date,
      amount: entry.amount,
      payeeCiphertext: payee.ciphertext,
      payeeKid: payee.kid,
      descCiphertext: desc.ciphertext,
      descKid: desc.kid,
    };
  });

  await prisma.bankLine.createMany({
    data: lines,
    skipDuplicates: true,
  });

  console.log("Seed OK");
}

main()
  .catch((e) => {
    console.error(e);
    process.exit(1);
  })
  .finally(async () => {
    await prisma.$disconnect();
  });



============================================================
FILE: C:\src\apgms-final\scripts\verify-designated-reconciliation.mjs
============================================================
import { prisma } from "@apgms/shared/db.js";

const TWENTY_FOUR_HOURS_MS = 24 * 60 * 60 * 1000;

async function main() {
  const since = new Date(Date.now() - TWENTY_FOUR_HOURS_MS);
  const artifact = await prisma.evidenceArtifact.findFirst({
    where: {
      kind: "designated-reconciliation",
      createdAt: { gte: since },
    },
    orderBy: { createdAt: "desc" },
  });

  if (!artifact) {
    throw new Error("No designated-reconciliation artefact generated within the last 24 hours");
  }

  console.info("designated-reconciliation artifact found", {
    artifactId: artifact.id,
    sha256: artifact.sha256,
    createdAt: artifact.createdAt,
  });
}

main()
  .catch((error) => {
    console.error("designated-reconciliation health check failed", error);
    process.exitCode = 1;
  })
  .finally(async () => {
    await prisma.$disconnect();
  });



============================================================
FILE: C:\src\apgms-final\scripts\verify-no-conflicts.mjs
============================================================
#!/usr/bin/env node
import { spawnSync } from "node:child_process";

const markers = [
  { pattern: "<<<<<<< ", label: "conflict start" },
  { pattern: "======= ", label: "conflict middle" },
  { pattern: ">>>>>>> ", label: "conflict end" },
];

let hasConflicts = false;

for (const marker of markers) {
  const args = [
    "grep",
    "-n",
    marker.pattern,
    "--",
    ".",
    ":(exclude)scripts/verify-no-conflicts.mjs",
  ];
  const result = spawnSync("git", args, {
    stdio: hasConflicts ? "ignore" : "inherit",
  });

  if (result.status === 0) {
    hasConflicts = true;
    continue;
  }

  if (result.status === 1) {
    continue;
  }

  console.error("git grep failed to execute; please ensure git is available.");
  process.exit(result.status ?? 2);
}

if (hasConflicts) {
  console.error("Detected merge conflict markers. Please resolve them before continuing.");
  process.exit(1);
}

process.exit(0);



============================================================
FILE: C:\src\apgms-final\SECURITY.md
============================================================
# Security Policy
Email: security@yourdomain.example



============================================================
FILE: C:\src\apgms-final\services\api-gateway\db\migrations\20251117122810_init_apgms_schema\migration.sql
============================================================
-- CreateEnum
CREATE TYPE "Schedule" AS ENUM ('MONTHLY', 'QUARTERLY');

-- CreateTable
CREATE TABLE "Organization" (
    "id" TEXT NOT NULL,
    "abn" TEXT NOT NULL,
    "legalName" TEXT NOT NULL,
    "createdByUserId" TEXT,
    "schedule" "Schedule" NOT NULL DEFAULT 'QUARTERLY',
    "shortfallThresholdBps" INTEGER NOT NULL DEFAULT 500,

    CONSTRAINT "Organization_pkey" PRIMARY KEY ("id")
);

-- CreateTable
CREATE TABLE "DesignatedAccount" (
    "id" TEXT NOT NULL,
    "orgId" TEXT NOT NULL,
    "type" TEXT NOT NULL,
    "provider" TEXT NOT NULL,
    "mandateId" TEXT,
    "displayName" TEXT,

    CONSTRAINT "DesignatedAccount_pkey" PRIMARY KEY ("id")
);

-- CreateTable
CREATE TABLE "ObligationHistory" (
    "id" TEXT NOT NULL,
    "orgId" TEXT NOT NULL,
    "type" TEXT NOT NULL,
    "period" TEXT NOT NULL,
    "cents" INTEGER NOT NULL,
    "source" TEXT NOT NULL,
    "createdAt" TIMESTAMP(3) NOT NULL DEFAULT CURRENT_TIMESTAMP,

    CONSTRAINT "ObligationHistory_pkey" PRIMARY KEY ("id")
);

-- CreateIndex
CREATE UNIQUE INDEX "Organization_abn_key" ON "Organization"("abn");

-- CreateIndex
CREATE UNIQUE INDEX "DesignatedAccount_orgId_type_key" ON "DesignatedAccount"("orgId", "type");

-- CreateIndex
CREATE INDEX "ObligationHistory_orgId_type_period_idx" ON "ObligationHistory"("orgId", "type", "period");

-- AddForeignKey
ALTER TABLE "DesignatedAccount" ADD CONSTRAINT "DesignatedAccount_orgId_fkey" FOREIGN KEY ("orgId") REFERENCES "Organization"("id") ON DELETE RESTRICT ON UPDATE CASCADE;

-- AddForeignKey
ALTER TABLE "ObligationHistory" ADD CONSTRAINT "ObligationHistory_orgId_fkey" FOREIGN KEY ("orgId") REFERENCES "Organization"("id") ON DELETE RESTRICT ON UPDATE CASCADE;



============================================================
FILE: C:\src\apgms-final\services\api-gateway\db\schema.prisma
============================================================
// services/api-gateway/db/schema.prisma

// services/api-gateway/db/schema.prisma
datasource db {
  provider          = "postgresql"
  url               = env("DATABASE_URL")
  shadowDatabaseUrl = env("SHADOW_DATABASE_URL")
}

generator client {
  provider = "prisma-client-js"
}

enum Schedule {
  MONTHLY
  QUARTERLY
}

model Organization {
  id        String @id @default(cuid())
  abn       String @unique
  legalName String

  // Who created this organisation record (onboarding user)
  createdByUserId String?

  // Schedule + shortfall threshold for secured tax buffers
  schedule              Schedule @default(QUARTERLY)
  shortfallThresholdBps Int      @default(500)

  designatedAccounts DesignatedAccount[]
  obligationHistory  ObligationHistory[]
}

model DesignatedAccount {
  id          String @id @default(cuid())
  orgId       String
  type        String
  provider    String
  mandateId   String?
  displayName String?

  organization Organization @relation(fields: [orgId], references: [id])

  // One designated account per org + tax type
  @@unique([orgId, type], name: "orgId_type")
}

model ObligationHistory {
  id        String   @id @default(cuid())
  orgId     String
  type      String   // "GST" | "PAYGW" | "PAYGI"
  period    String   // "YYYY-MM" or "YYYY-Qn"
  cents     Int
  source    String   // "import", "statement", "manual"
  createdAt DateTime @default(now())

  organization Organization @relation(fields: [orgId], references: [id])

  @@index([orgId, type, period])
}



============================================================
FILE: C:\src\apgms-final\services\api-gateway\db\seed-dev-admin.ts
============================================================
// db/seed-dev-admin.ts
import { PrismaClient } from "@prisma/client";
import bcrypt from "bcryptjs";

const prisma = new PrismaClient();

async function main() {
  const email = "dev@example.com";
  const password = "admin123";

  // Hash password using bcrypt (adjust rounds if you like)
  const passwordHash = await bcrypt.hash(password, 10);

  // âš ï¸ IMPORTANT:
  // Change field names below to match your `User` model in schema.prisma
  // Common pattern:
  // model User {
  //   id           String  @id @default(cuid())
  //   email        String  @unique
  //   passwordHash String
  //   role         String
  //   ...
  // }

  const user = await prisma.user.upsert({
    where: { email },          // <-- email field name
    update: {
      passwordHash,           // <-- password field name
      role: "ADMIN",          // <-- adjust to whatever your enum/string is
    },
    create: {
      email,                  // <-- email field name
      passwordHash,           // <-- password field name
      role: "ADMIN",          // <-- adjust
    },
  });

  console.log("Seeded dev admin user:", {
    id: user.id,
    email: user.email,
    role: (user as any).role,
  });
}

main()
  .catch((err) => {
    console.error("Failed to seed dev admin:", err);
    process.exit(1);
  })
  .finally(async () => {
    await prisma.$disconnect();
  });



============================================================
FILE: C:\src\apgms-final\services\api-gateway\db_schema.sql
============================================================
--
-- PostgreSQL database dump
--

\restrict 9XqhlHngdqpBh86c0IKbaoBoRRA5QSStLuEvKV5WpuZfieeFNts2MPbTafcLAW5

-- Dumped from database version 16.10 (Debian 16.10-1.pgdg13+1)
-- Dumped by pg_dump version 16.10 (Debian 16.10-1.pgdg13+1)

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: pgcrypto; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS pgcrypto WITH SCHEMA public;


--
-- Name: EXTENSION pgcrypto; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION pgcrypto IS 'cryptographic functions';


SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: AuditLog; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public."AuditLog" (
    id text NOT NULL,
    "orgId" text NOT NULL,
    "actorId" text NOT NULL,
    action text NOT NULL,
    metadata jsonb,
    "createdAt" timestamp(3) without time zone DEFAULT now() NOT NULL
);


ALTER TABLE public."AuditLog" OWNER TO postgres;

--
-- Name: BankLine; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public."BankLine" (
    id text NOT NULL,
    "orgId" text NOT NULL,
    date timestamp(3) without time zone NOT NULL,
    amount numeric(65,30) NOT NULL,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "idempotencyKey" text,
    "payeeCiphertext" text NOT NULL,
    "payeeKid" text NOT NULL,
    "descCiphertext" text NOT NULL,
    "descKid" text NOT NULL
);


ALTER TABLE public."BankLine" OWNER TO postgres;

--
-- Name: Org; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public."Org" (
    id text NOT NULL,
    name text NOT NULL,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "deletedAt" timestamp(3) without time zone
);


ALTER TABLE public."Org" OWNER TO postgres;

--
-- Name: OrgTombstone; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public."OrgTombstone" (
    id text NOT NULL,
    "orgId" text NOT NULL,
    payload jsonb NOT NULL,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL
);


ALTER TABLE public."OrgTombstone" OWNER TO postgres;

--
-- Name: User; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public."User" (
    id text NOT NULL,
    email text NOT NULL,
    password text NOT NULL,
    "createdAt" timestamp(3) without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    "orgId" text NOT NULL
);


ALTER TABLE public."User" OWNER TO postgres;

--
-- Name: AuditLog AuditLog_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public."AuditLog"
    ADD CONSTRAINT "AuditLog_pkey" PRIMARY KEY (id);


--
-- Name: BankLine BankLine_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public."BankLine"
    ADD CONSTRAINT "BankLine_pkey" PRIMARY KEY (id);


--
-- Name: OrgTombstone OrgTombstone_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public."OrgTombstone"
    ADD CONSTRAINT "OrgTombstone_pkey" PRIMARY KEY (id);


--
-- Name: Org Org_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public."Org"
    ADD CONSTRAINT "Org_pkey" PRIMARY KEY (id);


--
-- Name: User User_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public."User"
    ADD CONSTRAINT "User_pkey" PRIMARY KEY (id);


--
-- Name: AuditLog_actorId_createdAt_idx; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX "AuditLog_actorId_createdAt_idx" ON public."AuditLog" USING btree ("actorId", "createdAt");


--
-- Name: AuditLog_orgId_createdAt_idx; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX "AuditLog_orgId_createdAt_idx" ON public."AuditLog" USING btree ("orgId", "createdAt");


--
-- Name: BankLine_orgId_idempotencyKey_key; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX "BankLine_orgId_idempotencyKey_key" ON public."BankLine" USING btree ("orgId", "idempotencyKey");


--
-- Name: BankLine_orgId_idx; Type: INDEX; Schema: public; Owner: postgres
--

CREATE INDEX "BankLine_orgId_idx" ON public."BankLine" USING btree ("orgId");


--
-- Name: User_email_key; Type: INDEX; Schema: public; Owner: postgres
--

CREATE UNIQUE INDEX "User_email_key" ON public."User" USING btree (email);


--
-- Name: BankLine BankLine_orgId_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public."BankLine"
    ADD CONSTRAINT "BankLine_orgId_fkey" FOREIGN KEY ("orgId") REFERENCES public."Org"(id) ON UPDATE CASCADE ON DELETE CASCADE;


--
-- Name: OrgTombstone OrgTombstone_orgId_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public."OrgTombstone"
    ADD CONSTRAINT "OrgTombstone_orgId_fkey" FOREIGN KEY ("orgId") REFERENCES public."Org"(id) ON UPDATE CASCADE ON DELETE CASCADE;


--
-- Name: User User_orgId_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public."User"
    ADD CONSTRAINT "User_orgId_fkey" FOREIGN KEY ("orgId") REFERENCES public."Org"(id) ON UPDATE CASCADE ON DELETE CASCADE;


--
-- PostgreSQL database dump complete
--

\unrestrict 9XqhlHngdqpBh86c0IKbaoBoRRA5QSStLuEvKV5WpuZfieeFNts2MPbTafcLAW5




============================================================
FILE: C:\src\apgms-final\services\api-gateway\package.json
============================================================
{
  "name": "@apgms/api-gateway",
  "version": "0.1.0",
  "private": true,
  "type": "module",
  "scripts": {
    "build": "tsc -p tsconfig.json",
    "test": "jest --config ../../jest.config.js services/api-gateway/test",
    "typecheck": "tsc --noEmit -p tsconfig.json",
    "start": "node dist/index.js",
    "dev": "tsx watch src/index.ts",
    "prisma:migrate": "prisma migrate dev",
    "prisma:generate": "prisma generate"
  },
  "prisma": {
    "schema": "../../shared/prisma/schema.prisma"
  },
  "dependencies": {
    "@apgms/connectors": "workspace:*",
    "@apgms/domain-policy": "workspace:*",
    "@apgms/shared": "workspace:*",
    "@aws-sdk/client-secrets-manager": "^3.933.0",
    "@fastify/helmet": "^13.0.2",
    "@fastify/rate-limit": "^10.3.0",
    "@opentelemetry/api": "1.8.0",
    "@opentelemetry/exporter-trace-otlp-http": "^0.50.0",
    "@opentelemetry/resources": "^1.23.0",
    "@opentelemetry/sdk-node": "^0.50.0",
    "@opentelemetry/semantic-conventions": "^1.23.0",
    "@prisma/client": "6.19.0",
    "@simplewebauthn/server": "^13.2.2",
    "bcryptjs": "^3.0.3",
    "dotenv": "^17.2.3",
    "fastify-plugin": "^5.1.0",
    "jose": "^6.1.0",
    "jsonwebtoken": "^9.0.2",
    "prom-client": "^15.1.3",
    "redis": "^5.9.0"
  },
  "devDependencies": {
    "@types/bcryptjs": "^3.0.0",
    "@types/jsonwebtoken": "^9.0.10",
    "prisma": "6.19.0"
  }
}



============================================================
FILE: C:\src\apgms-final\services\api-gateway\pnpm-lock.yaml
============================================================
lockfileVersion: '9.0'

settings:
  autoInstallPeers: true
  excludeLinksFromLockfile: false

importers:

  .:
    dependencies:
      '@apgms/shared':
        specifier: workspace:*
        version: link:../../shared
      '@fastify/cors':
        specifier: ^11.1.0
        version: 11.1.0
      dotenv:
        specifier: ^16.6.1
        version: 16.6.1
      fastify:
        specifier: ^5.6.1
        version: 5.6.1
    devDependencies:
      '@types/node':
        specifier: ^24.7.1
        version: 24.7.1
      tsx:
        specifier: ^4.20.6
        version: 4.20.6
      typescript:
        specifier: ^5.9.3
        version: 5.9.3

packages:

  '@esbuild/aix-ppc64@0.25.10':
    resolution: {integrity: sha512-0NFWnA+7l41irNuaSVlLfgNT12caWJVLzp5eAVhZ0z1qpxbockccEt3s+149rE64VUI3Ml2zt8Nv5JVc4QXTsw==}
    engines: {node: '>=18'}
    cpu: [ppc64]
    os: [aix]

  '@esbuild/android-arm64@0.25.10':
    resolution: {integrity: sha512-LSQa7eDahypv/VO6WKohZGPSJDq5OVOo3UoFR1E4t4Gj1W7zEQMUhI+lo81H+DtB+kP+tDgBp+M4oNCwp6kffg==}
    engines: {node: '>=18'}
    cpu: [arm64]
    os: [android]

  '@esbuild/android-arm@0.25.10':
    resolution: {integrity: sha512-dQAxF1dW1C3zpeCDc5KqIYuZ1tgAdRXNoZP7vkBIRtKZPYe2xVr/d3SkirklCHudW1B45tGiUlz2pUWDfbDD4w==}
    engines: {node: '>=18'}
    cpu: [arm]
    os: [android]

  '@esbuild/android-x64@0.25.10':
    resolution: {integrity: sha512-MiC9CWdPrfhibcXwr39p9ha1x0lZJ9KaVfvzA0Wxwz9ETX4v5CHfF09bx935nHlhi+MxhA63dKRRQLiVgSUtEg==}
    engines: {node: '>=18'}
    cpu: [x64]
    os: [android]

  '@esbuild/darwin-arm64@0.25.10':
    resolution: {integrity: sha512-JC74bdXcQEpW9KkV326WpZZjLguSZ3DfS8wrrvPMHgQOIEIG/sPXEN/V8IssoJhbefLRcRqw6RQH2NnpdprtMA==}
    engines: {node: '>=18'}
    cpu: [arm64]
    os: [darwin]

  '@esbuild/darwin-x64@0.25.10':
    resolution: {integrity: sha512-tguWg1olF6DGqzws97pKZ8G2L7Ig1vjDmGTwcTuYHbuU6TTjJe5FXbgs5C1BBzHbJ2bo1m3WkQDbWO2PvamRcg==}
    engines: {node: '>=18'}
    cpu: [x64]
    os: [darwin]

  '@esbuild/freebsd-arm64@0.25.10':
    resolution: {integrity: sha512-3ZioSQSg1HT2N05YxeJWYR+Libe3bREVSdWhEEgExWaDtyFbbXWb49QgPvFH8u03vUPX10JhJPcz7s9t9+boWg==}
    engines: {node: '>=18'}
    cpu: [arm64]
    os: [freebsd]

  '@esbuild/freebsd-x64@0.25.10':
    resolution: {integrity: sha512-LLgJfHJk014Aa4anGDbh8bmI5Lk+QidDmGzuC2D+vP7mv/GeSN+H39zOf7pN5N8p059FcOfs2bVlrRr4SK9WxA==}
    engines: {node: '>=18'}
    cpu: [x64]
    os: [freebsd]

  '@esbuild/linux-arm64@0.25.10':
    resolution: {integrity: sha512-5luJWN6YKBsawd5f9i4+c+geYiVEw20FVW5x0v1kEMWNq8UctFjDiMATBxLvmmHA4bf7F6hTRaJgtghFr9iziQ==}
    engines: {node: '>=18'}
    cpu: [arm64]
    os: [linux]

  '@esbuild/linux-arm@0.25.10':
    resolution: {integrity: sha512-oR31GtBTFYCqEBALI9r6WxoU/ZofZl962pouZRTEYECvNF/dtXKku8YXcJkhgK/beU+zedXfIzHijSRapJY3vg==}
    engines: {node: '>=18'}
    cpu: [arm]
    os: [linux]

  '@esbuild/linux-ia32@0.25.10':
    resolution: {integrity: sha512-NrSCx2Kim3EnnWgS4Txn0QGt0Xipoumb6z6sUtl5bOEZIVKhzfyp/Lyw4C1DIYvzeW/5mWYPBFJU3a/8Yr75DQ==}
    engines: {node: '>=18'}
    cpu: [ia32]
    os: [linux]

  '@esbuild/linux-loong64@0.25.10':
    resolution: {integrity: sha512-xoSphrd4AZda8+rUDDfD9J6FUMjrkTz8itpTITM4/xgerAZZcFW7Dv+sun7333IfKxGG8gAq+3NbfEMJfiY+Eg==}
    engines: {node: '>=18'}
