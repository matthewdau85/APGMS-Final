# Training Data Governance Runbook

This runbook documents how ledger discrepancy, manual override, and payment-plan events are collected for model training without violating confidentiality obligations.

## Redacted Export Process

1. **Source tables**: `discrepancy_events`, `manual_resolutions`, `payment_plan_metadata` (Prisma models `DiscrepancyEvent`, `ManualResolution`, `PaymentPlanMetadata`).
2. **Export job**: `worker/src/jobs/event-ingestion.ts` writes JSON payloads with org-scoped context. Before sharing outside the compliance enclave:
   - Drop all raw payload fields that contain subject identifiers (TFNs, emails, account numbers). Only keep derived metrics and hashed identifiers.
   - Apply deterministic hashing (`SHA-256` truncated to 16 chars) to any user IDs referenced in `initiatedBy` / `resolvedBy`. Salt with the nightly rotating export secret.
   - Replace monetary amounts (`*_cents`) with bucketed bands: `<$10k`, `$10k-$50k`, `$50k-$250k`, `>$250k`. Keep the exact cent values internally for model training but not in external exports.
3. **Redaction manifest**: store the column-level redaction map in `artifacts/redaction-manifests/latest.json` with schema version `2025-11-05`.
4. **Export approval**: Ops lead must sign off each export in the audit log (`audit_log` table, action `training.export`).

## Retention & Deletion Policies

| Dataset | Primary Store | Retention | Notes |
|---------|---------------|-----------|-------|
| Raw discrepancy envelopes | JetStream stream `${NATS_SUBJECT_PREFIX || "apgms.events"}` | 14 days rolling | Stream configured with `RetentionPolicy.Limits`; drain via `nats stream rm --filter` once materialised. |
| Normalised training tables | Postgres (`discrepancy_events`, `manual_resolutions`, `payment_plan_metadata`) | 24 months | Required for longitudinal payment behaviour modelling. Records older than 24 months are purged monthly via `scripts/purge-training-data.mjs`. |
| Redacted exports | MinIO bucket `ml-training/redacted/` | 18 months | Retained for reproducibility; deletion script verifies downstream models have been retrained. |

When an org requests erasure:

1. Trigger the existing secure deletion flow (`/admin/delete/:orgId`).
2. Run `pnpm worker purge-training --org <id>` (future script) to delete associated records from the three training tables.
3. Purge redacted exports that reference the hashed org identifier.
4. Record completion in `audit_log` (`action = "training.deletion"`).

## Access Controls

- Only the ingestion worker service account may publish to the JetStream consumer durable `training-ledger`.
- The analytics cluster accesses Postgres through the readonly role `analytics_reader`, granted `SELECT` on the three training tables.
- Redacted exports require temporary credentials generated by the security team; credentials expire after 24 hours.

## Monitoring & Alerts

- JetStream backlog > 5,000 messages triggers `training.ingestion.backlog` alert (Prometheus metric `nats_stream_messages_pending`).
- Daily ingestion summary written by the worker logs to `metrics/training-intake.log`; check for anomalies >25% drop in records.
- Prisma errors during ingestion emit `event_ingestion_failed` logs; wire these into the SIEM.

