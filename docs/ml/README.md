# Machine Learning Standards

This document captures the stakeholder-approved acceptance criteria for the
initial APGMS machine learning workstream. Targets will be reviewed every
quarter with Risk and Finance.

## Feature Aggregation Scope

- **Ledger history** – 90-day lookback of ledger entries aggregated into
  monthly net flow, credit/debit velocity, and balance volatility statistics.
- **Payment punctuality** – capture on-time ratios, average days late, and
  payment cadence for the last 6 months of payroll activity.
- **Discrepancy intelligence** – combine the `LedgerDiscrepancy` and
  `PayrollDiscrepancy` tables to monitor open cases, resolution latency, and
  severity mix.

## Shortfall Risk Model (Gradient Boosting)

- **Training data**: feature vectors generated by
  `features.build_feature_vector` with at least 2 fiscal quarters of history.
- **Performance targets** (evaluated on a stratified hold-out set):
  - Precision ≥ **0.82** on employers flagged as high shortfall risk.
  - Recall ≥ **0.70** to ensure sufficient coverage of true shortfalls.
  - ROC AUC ≥ **0.85** as a stability guardrail.
- **Operational monitoring**:
  - Trigger review if weekly precision drops below 0.75 or recall below 0.65.
  - Alert if feature drift results in >10% relative change to average
    ledger net flow.

## Anomaly Detection Model (Isolation Forest)

- **Training data**: same aggregated features with optional analyst labels for
  historic anomalies.
- **Performance targets** (where ground truth labels exist):
  - Precision ≥ **0.60** on anomaly alerts routed to analysts.
  - Recall ≥ **0.55** to prevent missing systemic issues.
  - False positive rate ≤ **0.30** over a rolling 4-week window.
- **Operational monitoring**:
  - Raise incident if average anomaly score variance doubles week-over-week.
  - Escalate to reconciliation team if open discrepancy count exceeds the 95th
    percentile observed during model backtesting.

## Experiment Tracking

- All training and evaluation runs must log to the shared MLflow tracking
  directory at `artifacts/notebooks/mlruns`.
- Each run must append a structured entry to
  `artifacts/notebooks/baseline_runs.json` summarising metrics, parameters, and
  timestamps for auditability.
- Notebook analyses should embed links to the MLflow run IDs for quick replay.
